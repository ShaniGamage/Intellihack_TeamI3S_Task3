# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HgxpxYbYmy3R1JH3JKtlc8s7TyV8xZpJ
"""

!pip install transformers datasets torch bitsandbytes

def load_jsonl(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        data = []
        for i, line in enumerate(file):
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"Error on line {i+1}: {line.strip()}")
                print(f"Error details: {e}")
    return data

file_path = "/content/Trainingdata.jsonl"
data = load_jsonl(file_path)

!pip install transformers datasets

pip install torch torchvision

from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM
from datasets import Dataset
import torch

# Load the pre-trained model and tokenizer (Qwen 2.5 3B)
model_name = "Qwen/Qwen2.5-3B-Instruct"  # You can also use "Qwen/Qwen2.5-3B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Prepare the dataset (from the loaded jsonl)
def preprocess_data(data):
    return tokenizer(data['question'], truncation=True, padding="max_length", max_length=512)

# Convert the list of dicts into a Hugging Face dataset
train_data = Dataset.from_dict(data)
train_data = train_data.map(preprocess_data, batched=True)

# Set training arguments
training_args = TrainingArguments(
    output_dir="./fine_tuned_model",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    logging_dir='./logs',
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="steps",
    save_total_limit=2,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
)

# Start training
trainer.train()

!pip install bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import bitsandbytes as bnb

# Load pre-trained model and tokenizer
model_name = "Qwen/Qwen2.5-3B-Instruct"  # Example model
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Perform 4-bit quantization (using bitsandbytes or similar)
model = model.half()  # Convert to 16-bit precision first
model = model.quantize(4)  # Quantize to 4-bit precision

# Save model to a .gguf file (replace this part with the appropriate saving function for your environment)
model.save_pretrained("quantized_model.gguf")